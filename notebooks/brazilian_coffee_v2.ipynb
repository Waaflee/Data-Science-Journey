{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment configuration\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# env config\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"false\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"\"\n",
    "\n",
    "# gpu training optimization\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# tensorboard config\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "epochs = 100\n",
    "montecarlo_iterations = 25\n",
    "batch_size = 64\n",
    "img_height = 64\n",
    "img_width = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "HP_NUM_CONV_UNITS = hp.HParam('num_conv_units', hp.Discrete([8, 16]))\n",
    "HP_NUM_DENSE_UNITS = hp.HParam('num_dense_units', hp.Discrete([16, 32, 64, 128]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.7))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['Adam']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "METRIC_STD = 'std'\n",
    "\n",
    "\n",
    "# Hyper parameters\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "      hparams=[HP_NUM_CONV_UNITS, HP_NUM_DENSE_UNITS,\n",
    "               HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy'),\n",
    "             hp.Metric(METRIC_STD, display_name='STD')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "# custom layers\n",
    "# data augmentation layers (training only)\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\",\n",
    "                                   input_shape=(img_height,\n",
    "                                                img_width,\n",
    "                                                3)),\n",
    "        tf.keras.layers.RandomRotation(0.25),\n",
    "        tf.keras.layers.RandomZoom(0.25),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def create_model(hparams):\n",
    "    model = tf.keras.Sequential([\n",
    "        data_augmentation,\n",
    "        tf.keras.layers.Rescaling(\n",
    "            1./255, input_shape=(img_height, img_width, 3)),\n",
    "\n",
    "        tf.keras.layers.Conv2D(hparams[HP_NUM_CONV_UNITS], 3, activation='relu',),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hparams[HP_NUM_CONV_UNITS] * 2, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            hparams[HP_NUM_CONV_UNITS] * 3, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_DENSE_UNITS], activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(hparams[HP_OPTIMIZER],\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_ds, val_ds, model_name=\"\"):\n",
    "    # model.fit(train_ds, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "    model.fit(train_ds, epochs=epochs, callbacks=[tf.keras.callbacks.TensorBoard(log_dir=log_dir + model_name, histogram_freq=1)])\n",
    "    _, accuracy = model.evaluate(val_ds)\n",
    "    return accuracy                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte carlo cross validation\n",
    "\n",
    "data_path = \"/tf/data/train\"\n",
    "data_dir = pathlib.Path(data_path)\n",
    "test_dir = pathlib.Path(\"/tf/data/test\")\n",
    "\n",
    "\n",
    "def MCCV(model, hparams, hparams_log_dir):\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    # Monte carlo cross validation\n",
    "    for i in range(montecarlo_iterations):\n",
    "        # truly random dataset\n",
    "        seed = math.ceil(time.time())\n",
    "        # training dataset\n",
    "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "                data_dir,\n",
    "                validation_split=0.2,\n",
    "                subset=\"training\",\n",
    "                seed=seed,\n",
    "                image_size=(img_height, img_width),\n",
    "                batch_size=batch_size)\n",
    "        # validation dataset \n",
    "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "            data_dir,\n",
    "            validation_split=0.2,\n",
    "            subset=\"validation\",\n",
    "            seed=seed,\n",
    "            image_size=(img_height, img_width),\n",
    "            batch_size=batch_size)\n",
    "\n",
    "        # test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        #     test_dir,\n",
    "        #     seed=seed,\n",
    "        #     image_size=(img_height, img_width),\n",
    "        #     batch_size=batch_size)\n",
    "        LOG_NAME = \"model-{}-{}-{}\".format(hparams[HP_NUM_CONV_UNITS], hparams[HP_NUM_DENSE_UNITS], hparams[HP_DROPOUT])\n",
    "        accuracy = train_model(model, train_ds, val_ds, model_name=LOG_NAME)\n",
    "        all_scores.append(accuracy)\n",
    "\n",
    "    with tf.summary.create_file_writer(hparams_log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        tf.summary.scalar(METRIC_ACCURACY, np.mean(all_scores), step=1)\n",
    "        # tf.summary.scalar(METRIC_ACCURACY, all_scores, step=1)\n",
    "        tf.summary.scalar(METRIC_STD, np.std(all_scores), step=1)\n",
    "    \n",
    "    return (np.mean(all_scores), np.std(all_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    model_performance = []\n",
    "    session_num = 0\n",
    "    for num_conv_units in HP_NUM_CONV_UNITS.domain.values:\n",
    "        for num_dense_units in HP_NUM_DENSE_UNITS.domain.values:\n",
    "            for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "                for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                    hparams = {\n",
    "                        HP_NUM_CONV_UNITS: num_conv_units,\n",
    "                        HP_NUM_DENSE_UNITS: num_dense_units,\n",
    "                        HP_DROPOUT: dropout_rate,\n",
    "                        HP_OPTIMIZER: optimizer,\n",
    "                    }\n",
    "                    run_name = \"run-%d\" % session_num\n",
    "                    print('--- Starting trial: %s' % run_name)\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    model = create_model(hparams)\n",
    "                    metrics = MCCV(model, hparams, 'logs/hparam_tuning/' + run_name)\n",
    "                    model_performance.append(metrics)\n",
    "                    session_num += 1\n",
    "\n",
    "    return (model_performance, session_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_performances, sessions = grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(list(range(0 ,len(models_performances))), [x[0] for x in models_performances])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit --bind_all --port 6008\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/hparam_tuning --bind_all --port 6007"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
